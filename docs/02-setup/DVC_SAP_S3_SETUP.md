# DVC with SAP Object Store (AWS S3) Setup

## Overview

DVC (Data Version Control) is now configured to use SAP Object Store (AWS S3) for storing large files, models, and datasets. This enables efficient version control for ML models and large binary files without bloating the Git repository.

## Configuration

### DVC Remote Storage

**Location**: `.dvc/config`

```ini
[core]
    remote = sap_s3
    autostage = true

['remote "sap_s3"']
    url = s3://hcp-055af4b0-2344-40d2-88fe-ddc1c4aad6c5/dvc-storage
    region = us-east-1
    access_key_id = YOUR_AWS_ACCESS_KEY_ID
    secret_access_key = YOUR_AWS_SECRET_ACCESS_KEY
```

### SAP Object Store Details

- **Bucket**: `hcp-055af4b0-2344-40d2-88fe-ddc1c4aad6c5`
- **Region**: `us-east-1`
- **DVC Prefix**: `dvc-storage/`
- **Provider**: AWS S3-compatible via SAP BTP

## Usage

### Basic DVC Commands

#### 1. Add Files to DVC

```bash
# Add a large model file
dvc add models/qwen3-coder-30b.gguf

# This creates a .dvc file that tracks the model
# The actual file is uploaded to S3
```

#### 2. Push to Remote Storage

```bash
# Push all tracked files to SAP S3
dvc push

# Push specific file
dvc push models/qwen3-coder-30b.gguf.dvc
```

#### 3. Pull from Remote Storage

```bash
# Pull all tracked files from SAP S3
dvc pull

# Pull specific file
dvc pull models/qwen3-coder-30b.gguf.dvc
```

#### 4. Check Status

```bash
# See which files need to be pushed/pulled
dvc status

# Show remote status
dvc status --remote
```

#### 5. List Remote Files

```bash
# List all files in remote storage
dvc list . --dvc-only
```

### Workflow for Large Files

#### Adding New Models

```bash
# 1. Download or create model
wget https://example.com/model.gguf -O models/new-model.gguf

# 2. Add to DVC tracking
dvc add models/new-model.gguf

# 3. Commit the .dvc file to git
git add models/new-model.gguf.dvc models/.gitignore
git commit -m "feat: add new-model.gguf to DVC"

# 4. Push to SAP S3
dvc push

# 5. Push git changes
git push
```

#### Retrieving Models on New Machine

```bash
# 1. Clone repository
git clone https://github.com/your-org/your-repo.git
cd your-repo

# 2. Pull all DVC-tracked files from SAP S3
dvc pull

# Models are now available locally
ls models/
```

#### Updating Existing Models

```bash
# 1. Replace model file
cp new-version.gguf models/existing-model.gguf

# 2. Update DVC tracking
dvc add models/existing-model.gguf

# 3. Commit changes
git add models/existing-model.gguf.dvc
git commit -m "feat: update existing-model to v2"

# 4. Push to SAP S3
dvc push

# 5. Push git changes
git push
```

## Integration with serviceCore

### Model Storage Strategy

All large model files should be tracked with DVC:

```
models/
‚îú‚îÄ‚îÄ qwen3-coder-30b.gguf          # Tracked by DVC
‚îú‚îÄ‚îÄ qwen3-coder-30b.gguf.dvc      # Committed to Git
‚îú‚îÄ‚îÄ hy-mt-1.8b.gguf               # Tracked by DVC
‚îú‚îÄ‚îÄ hy-mt-1.8b.gguf.dvc           # Committed to Git
‚îî‚îÄ‚îÄ .gitignore                    # Generated by DVC
```

### Docker Integration

When building Docker images:

1. **Option A: Pull during build** (Recommended for CI/CD)
```dockerfile
# In Dockerfile
RUN dvc pull models/
```

2. **Option B: Mount volume** (Recommended for local dev)
```yaml
# In docker-compose.yml
volumes:
  - ./models:/app/models:ro
```

### GitHub Actions Integration

Add to `.github/workflows/docker-build-backend.yml`:

```yaml
- name: Pull DVC models
  env:
    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  run: |
    pip install dvc[s3]
    dvc pull
```

## Current Models Tracked

The following models are currently tracked with DVC:

| Model | Size | Purpose | DVC File |
|-------|------|---------|----------|
| Qwen3-Coder-30B | ~15GB | Code generation | `models/qwen3-coder.gguf.dvc` |
| HY-MT1.5-1.8B | ~2GB | Translation | `models/hy-mt.gguf.dvc` |
| CamelBERT | ~500MB | Arabic NLP | `models/camelbert.gguf.dvc` |
| DeepSeek-Math | ~7GB | Math reasoning | `models/deepseek-math.gguf.dvc` |

## Storage Organization

DVC files in SAP S3 are organized by content hash:

```
s3://hcp-055af4b0-2344-40d2-88fe-ddc1c4aad6c5/dvc-storage/
‚îú‚îÄ‚îÄ files/
‚îÇ   ‚îî‚îÄ‚îÄ md5/
‚îÇ       ‚îú‚îÄ‚îÄ ab/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ 1234567890abcdef1234567890abcdef  # Actual file content
‚îÇ       ‚îî‚îÄ‚îÄ cd/
‚îÇ           ‚îî‚îÄ‚îÄ fedcba0987654321fedcba0987654321
‚îî‚îÄ‚îÄ tmp/
```

## Troubleshooting

### Authentication Errors

```bash
# Verify AWS credentials
aws s3 ls s3://hcp-055af4b0-2344-40d2-88fe-ddc1c4aad6c5/ \
  --region us-east-1

# Re-configure DVC remote
dvc remote modify sap_s3 access_key_id YOUR_AWS_ACCESS_KEY_ID
dvc remote modify sap_s3 secret_access_key 'YOUR_AWS_SECRET_ACCESS_KEY'
```

### Slow Transfers

```bash
# Enable parallel transfers
dvc config cache.type copy
dvc config cache.protected true

# Increase connection pool
dvc remote modify sap_s3 ssl_verify false  # Only if behind proxy
```

### Cache Issues

```bash
# Clear local DVC cache
rm -rf .dvc/cache

# Re-pull from remote
dvc pull --force
```

### Check Remote Connectivity

```bash
# Test S3 connection
dvc remote list
dvc status --remote

# Verify specific file
dvc status models/qwen3-coder.gguf.dvc --remote
```

## Best Practices

### 1. **Track Large Files Only**
- Use DVC for files > 10MB
- Keep small config files in Git
- Never track both with Git and DVC

### 2. **Organized Directory Structure**
```
models/
‚îú‚îÄ‚îÄ llm/              # Large language models
‚îú‚îÄ‚îÄ embedding/        # Embedding models  
‚îú‚îÄ‚îÄ classification/   # Classification models
‚îî‚îÄ‚îÄ tokenizers/       # Tokenizer files
```

### 3. **Versioning Strategy**
- Use descriptive commit messages
- Tag important model versions
- Document model performance metrics

### 4. **CI/CD Integration**
- Always `dvc pull` before building
- Cache DVC files in CI pipeline
- Use `dvc push` after training

### 5. **Team Collaboration**
- Communicate before updating shared models
- Use branches for experimental models
- Document model requirements

## Environment Variables

DVC will automatically use credentials from:

1. **Environment variables** (highest priority)
```bash
export AWS_ACCESS_KEY_ID=YOUR_AWS_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=YOUR_AWS_SECRET_ACCESS_KEY
export AWS_DEFAULT_REGION=us-east-1
```

2. **DVC config file** (`.dvc/config`)
3. **AWS credentials file** (`~/.aws/credentials`)

## Migration from Local Storage

If you have existing models in local DVC cache:

```bash
# 1. Verify local cache
dvc cache dir

# 2. Push to SAP S3
dvc push --all-commits

# 3. Verify remote
dvc status --remote

# 4. Clean local cache (optional)
dvc gc --workspace --cloud
```

## Security Notes

‚ö†Ô∏è **Important Security Considerations:**

1. **Credentials in Config**: The `.dvc/config` file contains S3 credentials
   - ‚úÖ This file is in `.gitignore` by default
   - ‚ö†Ô∏è Never commit credentials to public repositories
   - üí° Consider using AWS IAM roles for production

2. **Alternative: Environment Variables**
```bash
# Remove credentials from .dvc/config
dvc remote modify sap_s3 --unset access_key_id
dvc remote modify sap_s3 --unset secret_access_key

# Use environment variables instead
export AWS_ACCESS_KEY_ID=YOUR_AWS_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY='YOUR_AWS_SECRET_ACCESS_KEY'
```

3. **GitHub Secrets**: For CI/CD, use GitHub Secrets
```yaml
env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
```

## Monitoring

### Storage Usage

```bash
# Check remote storage size
aws s3 ls s3://hcp-055af4b0-2344-40d2-88fe-dvc1c4aad6c5/dvc-storage/ \
  --recursive --human-readable --summarize

# Check local cache size
du -sh .dvc/cache
```

### DVC Metrics

```bash
# Show DVC statistics
dvc metrics show

# Show DVC pipeline
dvc dag
```

## Support

For issues or questions:
- **DVC Documentation**: https://dvc.org/doc
- **AWS S3 Documentation**: https://docs.aws.amazon.com/s3/
- **SAP BTP Documentation**: https://help.sap.com/docs/btp

---

**Last Updated**: January 22, 2026  
**DVC Version**: 3.x  
**Storage**: SAP Object Store (AWS S3)
