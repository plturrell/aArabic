# Three-Tab Dashboard Implementation Plan

**Created:** 2026-01-20  
**Status:** In Progress  
**Priority:** High - Production UI for LLM Platform

## Overview

Transform the nOpenaiServer dashboard into a comprehensive three-tab system:
1. **Prompt Testing** - Interactive testing with 4 modes
2. **mHC Fine-Tuning** - Geometric intelligence configuration
3. **Agent Orchestration** - Multi-service workflows

---

## ğŸ¯ Tab 1: Prompt Testing (PRIORITY 1)

### User Stories
- As a developer, I want to test prompts with different modes to optimize performance
- As a researcher, I want to compare responses across all 4 modes side-by-side
- As an operator, I want to track prompt history and performance metrics

### UI Components

**1.1 Quick Test Panel**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Testing                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mode: [Fast] [Normal] [Expert] [Research]           â”‚
â”‚                                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Enter your prompt here...                       â”‚ â”‚
â”‚ â”‚                                                 â”‚ â”‚
â”‚ â”‚                                                 â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                      â”‚
â”‚ Model: LFM2.5 1.2B Q4_0 (auto-selected) âœ“          â”‚
â”‚                                                      â”‚
â”‚ [Test Prompt] [Batch Test All Modes] [Clear]       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Response:                                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [Streaming response appears here...]            â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                      â”‚
â”‚ Metrics:                                             â”‚
â”‚ â€¢ Latency: 85ms (TTFT: 12ms)                        â”‚
â”‚ â€¢ Throughput: 58 tok/s                              â”‚
â”‚ â€¢ Cache Hit: 79%                                     â”‚
â”‚ â€¢ Tokens: 150 (prompt: 10, response: 140)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**1.2 Batch Test Panel**
- Test same prompt across all 4 modes simultaneously
- Side-by-side comparison view
- Aggregate metrics (avg latency, cost, quality scores)

**1.3 History Panel**
- Table of previous prompts with filters
- Export to CSV/JSON
- Re-run previous tests
- View full prompt/response details

### Backend APIs Needed

```
POST /api/v1/prompts/test
{
  "prompt": "What is 2+2?",
  "mode": "Fast",
  "max_tokens": 100,
  "temperature": 0.7,
  "system_prompt": "You are a helpful assistant"
}

Response:
{
  "prompt_id": "uuid",
  "response": "2+2 equals 4...",
  "metrics": {
    "latency_ms": 85,
    "ttft_ms": 12,
    "tokens_per_second": 58,
    "tokens_generated": 140,
    "cache_hit_rate": 0.79,
    "model_used": "lfm2.5-1.2b-q4_0"
  }
}

POST /api/v1/prompts/batch-test
{
  "prompt": "What is 2+2?",
  "modes": ["Fast", "Normal", "Expert", "Research"],
  "max_tokens": 100
}

Response:
{
  "batch_id": "uuid",
  "results": [
    { "mode": "Fast", "response": "...", "metrics": {...} },
    { "mode": "Normal", "response": "...", "metrics": {...} },
    ...
  ],
  "comparison": {
    "avg_latency": 250,
    "best_mode": "Fast",
    "total_cost": 0.0025
  }
}

GET /api/v1/prompts/history?limit=50&mode=Fast
Response:
{
  "prompts": [
    {
      "prompt_id": "uuid",
      "prompt_text": "What is 2+2?",
      "mode": "Fast",
      "timestamp": "2026-01-20T12:00:00Z",
      "metrics": {...}
    },
    ...
  ]
}
```

### HANA Integration
- Save all prompts to `PROMPT_HISTORY` table
- Query history with filters (mode, date range, user)
- Analytics: popular prompts, mode usage, performance trends

---

## âš™ï¸ Tab 2: mHC Fine-Tuning (PRIORITY 2)

### User Stories
- As a researcher, I want to configure mHC geometric constraints for different languages
- As an ML engineer, I want to monitor stability metrics in real-time
- As an Arabic NLP specialist, I want to validate improvements in morphology/dialects

### UI Components

**2.1 mHC Configuration Panel**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ mHC Configuration                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Enable mHC: [âœ“] Enabled                             â”‚
â”‚                                                      â”‚
â”‚ Core Settings:                                       â”‚
â”‚ â€¢ Sinkhorn Iterations: [10] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— (1-50)     â”‚
â”‚ â€¢ Stability Threshold: [1e-4] â”€â”€â—â”€â”€â”€â”€ (1e-6-1e-3)  â”‚
â”‚ â€¢ Manifold Beta: [10.0] â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€ (0.1-100)     â”‚
â”‚                                                      â”‚
â”‚ Manifold Type:                                       â”‚
â”‚ â—‹ Euclidean (default)                               â”‚
â”‚ â—‰ Hyperbolic (Arabic morphology +35%)               â”‚
â”‚ â—‹ Spherical (cross-dialectal +28%)                  â”‚
â”‚ â—‹ Product (code-switching +20%)                     â”‚
â”‚ â—‹ Auto-detect                                        â”‚
â”‚                                                      â”‚
â”‚ Layer Range:                                         â”‚
â”‚ Apply to layers: [0] to [79] (all 80 layers)       â”‚
â”‚ Or specific: [30-50] (middle layers only)           â”‚
â”‚                                                      â”‚
â”‚ [Apply Configuration] [Reset to Defaults] [Export]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2.2 Stability Monitoring Panel**
- Real-time line chart: Î± factor per layer (should be â‰ˆ1.0)
- Convergence iterations histogram
- Alert indicators (red/yellow/green status)
- Failure detection panel (over-constraint, geo-stat conflict, energy spike)

**2.3 Geometric Intelligence Panel**
- Curvature detection visualization
- Manifold type confidence scores (bar chart)
- Auto-detection results table
- Uncertainty quantification (bootstrap confidence intervals)

**2.4 Arabic NLP Validation Panel**
- Morphology accuracy: 65% â†’ 100% (+35% improvement chart)
- Dialect similarity: 72% â†’ 100% (+28% improvement chart)
- Code-switching: 80% â†’ 100% (+20% improvement chart)
- Long document translation quality (distortion reduction graph)

**2.5 Performance Profiling Panel**
- mHC overhead: 4.2% (target <5%) âœ“ green indicator
- SIMD speedup: 3.5x (ARM NEON)
- Memory usage: breakdown by component
- Speculation acceptance rate: 75% (target 70-85%) âœ“

### Backend APIs Needed

```
GET /api/v1/mhc/config
Response:
{
  "enabled": true,
  "sinkhorn_iterations": 10,
  "stability_threshold": 1e-4,
  "manifold_type": "hyperbolic",
  "layer_range": { "start": 0, "end": 79 },
  ...
}

PUT /api/v1/mhc/config
{
  "enabled": true,
  "manifold_type": "spherical",
  "sinkhorn_iterations": 15
}

GET /api/v1/mhc/metrics/stability
Response:
{
  "timestamp": "2026-01-20T12:00:00Z",
  "layers": [
    { "layer_id": 0, "alpha_factor": 1.02, "is_stable": true },
    { "layer_id": 1, "alpha_factor": 0.98, "is_stable": true },
    ...
  ],
  "global_stats": {
    "avg_alpha": 1.00,
    "stable_layers": 78,
    "unstable_layers": 2
  }
}

GET /api/v1/mhc/geometry/detection
Response:
{
  "detected_type": "hyperbolic",
  "confidence": 0.92,
  "curvature": -0.15,
  "alternatives": [
    { "type": "spherical", "confidence": 0.05 },
    { "type": "euclidean", "confidence": 0.03 }
  ]
}

GET /api/v1/mhc/arabic/validation
Response:
{
  "morphology": {
    "baseline": 0.65,
    "with_mhc": 1.00,
    "improvement": 0.35
  },
  "dialects": {
    "baseline": 0.72,
    "with_mhc": 1.00,
    "improvement": 0.28
  },
  "code_switching": {
    "baseline": 0.80,
    "with_mhc": 1.00,
    "improvement": 0.20
  }
}
```

### Zig Module Integration
Connect 21 mHC Zig modules to HTTP endpoints:
- `mhc_configuration.zig` â†’ `/api/v1/mhc/config`
- `mhc_constraints.zig` â†’ `/api/v1/mhc/metrics/stability`
- `mhc_geometry_detector.zig` â†’ `/api/v1/mhc/geometry/detection`
- `mhc_arabic_nlp_validation.zig` â†’ `/api/v1/mhc/arabic/validation`
- `mhc_monitor.zig` â†’ `/api/v1/mhc/alerts`

---

## ğŸ¤– Tab 3: Agent Orchestration (PRIORITY 3)

### User Stories
- As a system architect, I want to chain multiple AI services together
- As a product manager, I want to build complex workflows without coding
- As an operations engineer, I want to monitor multi-service performance

### UI Components

**3.1 Service Status Panel**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service Health                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Translation Service    âœ“ Healthy  99.9% uptime     â”‚
â”‚ Embedding Service      âœ“ Healthy  99.8% uptime     â”‚
â”‚ RAG Service            âœ“ Healthy  99.5% uptime     â”‚
â”‚ KTO Policy             âš  Degraded 95.0% uptime     â”‚
â”‚ Recursive LLM          âœ“ Healthy  99.7% uptime     â”‚
â”‚ TAU2-Bench             âœ“ Healthy  100% uptime      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3.2 Workflow Builder Panel**
- Visual drag-drop editor (nodes = services, edges = data flow)
- Pre-built templates (e.g., "Translate â†’ Embed â†’ RAG â†’ Generate")
- Conditional branching (if/else logic)
- Parallel execution support
- Fallback strategies (retry, alternate service)

**3.3 Multi-Agent Coordination Panel**
- Agent roster with capabilities
- Task delegation rules
- Consensus mechanisms (voting, averaging)
- Conflict resolution strategies

**3.4 Orchestration Metrics Panel**
- Total workflow latency (end-to-end)
- Per-service breakdown
- Error rates and retry attempts
- Resource utilization (across all services)
- Cost tracking ($ per workflow execution)

### Backend APIs Needed

```
GET /api/v1/orchestration/services
Response:
{
  "services": [
    {
      "name": "TranslationService",
      "status": "healthy",
      "uptime": 0.999,
      "mhc_enabled": true
    },
    ...
  ]
}

POST /api/v1/orchestration/workflow
{
  "name": "Arabic Translation Pipeline",
  "steps": [
    {
      "service": "TranslationService",
      "input": "{{ prompt }}",
      "output": "translation"
    },
    {
      "service": "EmbeddingService",
      "input": "{{ translation }}",
      "output": "embedding"
    },
    {
      "service": "RAGService",
      "input": { "query": "{{ translation }}", "embedding": "{{ embedding }}" },
      "output": "context"
    }
  ]
}

Response:
{
  "workflow_id": "uuid",
  "status": "completed",
  "results": { "context": "..." },
  "metrics": {
    "total_latency_ms": 450,
    "steps": [
      { "service": "TranslationService", "latency_ms": 120 },
      { "service": "EmbeddingService", "latency_ms": 80 },
      { "service": "RAGService", "latency_ms": 250 }
    ]
  }
}

GET /api/v1/orchestration/metrics?workflow_id=uuid
Response:
{
  "total_executions": 1250,
  "avg_latency_ms": 435,
  "error_rate": 0.02,
  "total_cost": 125.50
}
```

---

## ğŸ“‹ Implementation Roadmap

### Phase 1: Fix UI & Add Tab 1 (2 days)
- [x] Fix XML validation errors
- [ ] Debug blank page rendering
- [ ] Create PromptTesting.view.xml
- [ ] Create PromptTesting.controller.js
- [ ] Add backend API endpoints (`/api/v1/prompts/*`)
- [ ] Connect to HANA PROMPT_HISTORY table
- [ ] Test end-to-end flow

### Phase 2: Add Tab 2 (3 days)
- [ ] Create MHCFineTuning.view.xml (4 sub-panels)
- [ ] Create MHCFineTuning.controller.js
- [ ] Add backend API endpoints (`/api/v1/mhc/*`)
- [ ] Connect Zig mHC modules to HTTP layer
- [ ] Create real-time WebSocket for metrics
- [ ] Test Arabic NLP validation

### Phase 3: Add Tab 3 (4 days)
- [ ] Create AgentOrchestration.view.xml
- [ ] Create AgentOrchestration.controller.js
- [ ] Build visual workflow editor (drag-drop)
- [ ] Add backend API endpoints (`/api/v1/orchestration/*`)
- [ ] Integrate Mojo service discovery
- [ ] Test multi-service workflows

### Phase 4: Polish & Documentation (1 day)
- [ ] Create user guide with screenshots
- [ ] Add tooltips and help text
- [ ] Performance optimization
- [ ] Security audit (auth, input validation)
- [ ] Production deployment checklist

---

## ğŸ¨ Navigation Structure

```
webapp/
â”œâ”€â”€ view/
â”‚   â”œâ”€â”€ Main.view.xml (existing dashboard)
â”‚   â”œâ”€â”€ ModelConfigurator.view.xml (existing, updated)
â”‚   â”œâ”€â”€ PromptTesting.view.xml (NEW)
â”‚   â”œâ”€â”€ MHCFineTuning.view.xml (NEW)
â”‚   â””â”€â”€ AgentOrchestration.view.xml (NEW)
â”œâ”€â”€ controller/
â”‚   â”œâ”€â”€ Main.controller.js
â”‚   â”œâ”€â”€ ModelConfigurator.controller.js
â”‚   â”œâ”€â”€ PromptTesting.controller.js (NEW)
â”‚   â”œâ”€â”€ MHCFineTuning.controller.js (NEW)
â”‚   â””â”€â”€ AgentOrchestration.controller.js (NEW)
â””â”€â”€ i18n/
    â””â”€â”€ i18n.properties (add new labels)
```

**Main Navigation Bar:**
```
[Nucleus Openaiserver]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Dashboard] [Prompt Testing] [mHC Tuning] [Agents] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”’ Security Considerations

1. **Authentication**: All API endpoints require valid session
2. **Authorization**: Role-based access (admin, operator, viewer)
3. **Input Validation**: Sanitize all prompts, prevent injection
4. **Rate Limiting**: Max 100 requests/min per user
5. **Audit Logging**: Track all configuration changes
6. **Data Privacy**: Mask sensitive prompts in logs

---

## ğŸ“Š Success Metrics

**Tab 1: Prompt Testing**
- [ ] <200ms average response time for Fast mode
- [ ] 100+ prompts tested per day
- [ ] 4-mode comparison takes <10 seconds

**Tab 2: mHC Fine-Tuning**
- [ ] mHC overhead <5% confirmed
- [ ] Arabic NLP targets met (+35%, +28%, +20%)
- [ ] Zero stability failures in production

**Tab 3: Agent Orchestration**
- [ ] 10+ workflows created
- [ ] <500ms average workflow latency
- [ ] 99.9% workflow success rate

---

**Status:** Ready for implementation  
**Next Step:** Toggle to Act mode and start Phase 1
