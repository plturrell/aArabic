#!/usr/bin/env python3
"""
Quick benchmark runner - works with actual models

This script:
1. Checks model availability
2. Creates test data
3. Runs benchmarks
4. Reports results
"""

import sys
from pathlib import Path
import json

print("ğŸ” Checking system setup...")

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

# Check model directory
MODELS_DIR = Path("../../../vendor/layerModels/folderRepos/arabic_models")
print(f"\nğŸ“ Checking models directory: {MODELS_DIR}")

if MODELS_DIR.exists():
    print("âœ… Models directory found")
    models = [d.name for d in MODELS_DIR.iterdir() if d.is_dir()]
    print(f"   Available models: {', '.join(models)}")
else:
    print(f"âš ï¸  Models directory not found at {MODELS_DIR}")
    print("   Using fallback mode (metrics only, no translation)")

# Create test data
print("\nğŸ“ Creating test dataset...")

test_pairs = [
    ("Ø§Ù„ÙØ§ØªÙˆØ±Ø© Ø±Ù‚Ù… Ù¡Ù¢Ù£Ù¤", "Invoice number 1234"),
    ("Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù¡Ù Ù Ù  Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ", "Total amount 1000 Saudi Riyals"),
    ("ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚ Ù£Ù¡ ÙŠÙ†Ø§ÙŠØ± Ù¢Ù Ù¢Ù¥", "Due date January 31, 2025"),
    ("Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ Ù„Ù„Ù…ÙˆØ±Ø¯", "Supplier tax identification number"),
    ("Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©: Ø´Ø±ÙƒØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©", "Company name: Advanced Technology Company"),
    ("Ø§Ù„Ø¨Ù†Ùƒ: Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„ÙˆØ·Ù†ÙŠ", "Bank: National Bank"),
    ("Ø±Ù‚Ù… Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨Ù†ÙƒÙŠ", "Bank account number"),
    ("Ø¶Ø±ÙŠØ¨Ø© Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ©", "Value Added Tax"),
    ("Ø´Ø±ÙˆØ· Ø§Ù„Ø¯ÙØ¹: Ø«Ù„Ø§Ø«ÙˆÙ† ÙŠÙˆÙ…Ø§Ù‹", "Payment terms: thirty days"),
    ("Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©", "Address: Riyadh, Saudi Arabia"),
]

print(f"âœ… Created {len(test_pairs)} test pairs")

# Create CSV for benchmark
test_file = Path("data/translation_training/test_set.csv")
test_file.parent.mkdir(parents=True, exist_ok=True)

import csv
with open(test_file, 'w', encoding='utf-8', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['arabic', 'english', 'dialect', 'domain'])
    for arabic, english in test_pairs:
        # Infer domain
        domain = 'financial' if any(term in arabic for term in ['ÙØ§ØªÙˆØ±Ø©', 'Ø¶Ø±ÙŠØ¨Ø©', 'Ø±ÙŠØ§Ù„']) else 'general'
        writer.writerow([arabic, english, 'msa', domain])

print(f"âœ… Saved test set to {test_file}")

# Check if we can actually run translation
print("\nğŸ”§ Checking translation capabilities...")

try:
    # Try importing translation system
    from translation_system import ArabicTranslationSystem, TranslationInput
    
    print("âœ… Translation system imported")
    
    # Try to initialize (this will check services)
    print("\nğŸš€ Initializing translation system...")
    print("   (This will check CamelBERT, LocalAI, and Lean4 services)")
    print("   Note: Services may not be running - that's OK for metrics testing")
    
    system = ArabicTranslationSystem()
    print("âœ… System initialized")
    
    CAN_TRANSLATE = True
    
except Exception as e:
    print(f"âš ï¸  Translation system unavailable: {e}")
    print("   Will run metrics-only mode")
    CAN_TRANSLATE = False

# Check benchmark system
print("\nğŸ“Š Checking benchmark system...")

try:
    from metrics_benchmarks import (
        TranslationBenchmarkSuite,
        TranslationMetricsCalculator,
        BenchmarkVisualizer
    )
    print("âœ… Benchmark system imported")
    
    # Initialize
    suite = TranslationBenchmarkSuite()
    print("âœ… Benchmark suite initialized")
    
    metrics_calc = TranslationMetricsCalculator()
    print("âœ… Metrics calculator initialized")
    
    CAN_BENCHMARK = True
    
except Exception as e:
    print(f"âŒ Benchmark system error: {e}")
    CAN_BENCHMARK = False
    sys.exit(1)

# Decision point
print("\n" + "="*70)
print(" SYSTEM STATUS ".center(70))
print("="*70)
print(f"\n   Translation: {'âœ… Ready' if CAN_TRANSLATE else 'âš ï¸  Unavailable (services not running)'}")
print(f"   Benchmarking: {'âœ… Ready' if CAN_BENCHMARK else 'âŒ Failed'}")
print(f"   Test Data: âœ… Ready ({len(test_pairs)} pairs)")
print(f"   Models: {'âœ… Found' if MODELS_DIR.exists() else 'âš ï¸  Not found'}")

if not CAN_BENCHMARK:
    print("\nâŒ Cannot run benchmarks - fix errors above")
    sys.exit(1)

# Run appropriate mode
print("\n" + "="*70)

if CAN_TRANSLATE:
    print(" RUNNING FULL BENCHMARK (Translation + Metrics) ".center(70))
    print("="*70)
    
    print("\nâš ï¸  Note: This requires CamelBERT and LocalAI services")
    print("   If services are not running, benchmark will use fallback mode")
    
    input("\nPress Enter to continue or Ctrl+C to cancel...")
    
    try:
        # Run full benchmark
        result = suite.run_benchmark(
            test_pairs=test_pairs,
            benchmark_name="production_benchmark"
        )
        
        print("\nâœ… Benchmark completed successfully!")
        print(f"\nğŸ“Š Results: BLEU={result.avg_bleu:.1f}, Confidence={result.avg_confidence:.1%}")
        
        # Try to visualize
        try:
            print("\nğŸ“Š Generating visualizations...")
            viz = BenchmarkVisualizer()
            viz.plot_quality_distribution(result)
            viz.plot_model_comparison(result)
            print("âœ… Visualizations saved")
        except Exception as e:
            print(f"âš ï¸  Visualization failed (optional): {e}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Benchmark cancelled by user")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

else:
    print(" RUNNING METRICS-ONLY MODE ".center(70))
    print("="*70)
    
    print("\nğŸ“Š Computing metrics on test translations...")
    print("   (Using reference translations as 'system output' for demo)")
    
    all_metrics = []
    
    for i, (arabic, english) in enumerate(test_pairs, 1):
        print(f"\n[{i}/{len(test_pairs)}] {arabic[:40]}...")
        
        # Calculate metrics (using reference as both hypothesis and reference)
        metrics = metrics_calc.calculate_all_metrics(
            source_arabic=arabic,
            translated_english=english,  # Using reference as translation
            reference_english=english,
            dialect_confidence=0.9,
            grammar_score=0.85,
            model_used="reference",
            dialect_detected="msa",
            overall_quality="high",
            overall_confidence=0.95,
            translation_time_ms=0
        )
        
        all_metrics.append(metrics)
        
        print(f"   BLEU: {metrics.bleu_score:.1f}")
        print(f"   Financial Accuracy: {metrics.financial_term_accuracy:.1%}")
    
    # Calculate averages
    avg_bleu = sum(m.bleu_score for m in all_metrics) / len(all_metrics)
    avg_meteor = sum(m.meteor_score for m in all_metrics) / len(all_metrics)
    avg_financial = sum(m.financial_term_accuracy for m in all_metrics) / len(all_metrics)
    
    print("\n" + "="*70)
    print(" METRICS-ONLY RESULTS ".center(70))
    print("="*70)
    print(f"\n   Average BLEU: {avg_bleu:.1f}")
    print(f"   Average METEOR: {avg_meteor:.3f}")
    print(f"   Average Financial Accuracy: {avg_financial:.1%}")
    
    # Save results
    results_file = Path("benchmarks/translation/metrics_only_results.json")
    results_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'mode': 'metrics_only',
            'test_pairs': len(test_pairs),
            'avg_bleu': avg_bleu,
            'avg_meteor': avg_meteor,
            'avg_financial_accuracy': avg_financial,
            'note': 'Reference translations used as system output (demo mode)'
        }, f, indent=2)
    
    print(f"\nğŸ’¾ Results saved to {results_file}")

print("\n" + "="*70)
print(" COMPLETE ".center(70))
print("="*70)

print("\nğŸ“ Next Steps:")
print("   1. To run with real models, start CamelBERT and LocalAI services")
print("   2. To add more test data, edit data/translation_training/test_set.csv")
print("   3. To view results, check benchmarks/translation/ directory")
print("   4. To visualize, run with --visualize flag")

print("\nâœ… Benchmark system is ready!")
