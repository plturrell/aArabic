const std = @import("std");
const bpe = @import("bpe_tokenizer");

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();
    
    std.debug.print("\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    std.debug.print("  BPE TOKENIZER TESTS\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    
    // Test with Qwen3 tokenizer files
    const base_path = "vendor/layerModels/huggingFace/Qwen/Qwen3-Coder-30B-A3B-Instruct";
    
    const vocab_path = try std.fs.path.join(
        allocator,
        &[_][]const u8{ base_path, "vocab.json" },
    );
    defer allocator.free(vocab_path);
    
    const merges_path = try std.fs.path.join(
        allocator,
        &[_][]const u8{ base_path, "merges.txt" },
    );
    defer allocator.free(merges_path);
    
    std.debug.print("\nðŸ§ª Testing with Qwen3-Coder-30B tokenizer\n", .{});
    std.debug.print("   Vocab: {s}\n", .{vocab_path});
    std.debug.print("   Merges: {s}\n", .{merges_path});
    
    var tokenizer = bpe.BPETokenizer.init(allocator);
    defer tokenizer.deinit();
    
    // Load vocabulary and merges
    try tokenizer.loadVocab(vocab_path);
    try tokenizer.loadMerges(merges_path);
    
    std.debug.print("\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    std.debug.print("  TOKENIZER STATISTICS\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    
    std.debug.print("\nðŸ“Š Vocabulary:\n", .{});
    std.debug.print("   Total tokens: {d}\n", .{tokenizer.vocabSize()});
    std.debug.print("   BPE merges: {d}\n", .{tokenizer.merges.count()});
    
    std.debug.print("\nðŸŽ« Special Tokens:\n", .{});
    std.debug.print("   BOS (Begin of Sequence): {d}\n", .{tokenizer.bos_token_id});
    std.debug.print("   EOS (End of Sequence): {d}\n", .{tokenizer.eos_token_id});
    std.debug.print("   UNK (Unknown): {d}\n", .{tokenizer.unk_token_id});
    if (tokenizer.pad_token_id) |pad| {
        std.debug.print("   PAD (Padding): {d}\n", .{pad});
    } else {
        std.debug.print("   PAD (Padding): None\n", .{});
    }
    
    // Show some sample tokens
    std.debug.print("\nðŸ“ Sample Tokens (first 20):\n", .{});
    var count: usize = 0;
    var it = tokenizer.vocab.id_to_token.iterator();
    while (it.next()) |entry| {
        if (count >= 20) break;
        
        const id = entry.key_ptr.*;
        const token = entry.value_ptr.*;
        
        // Print token (show unprintable chars as hex)
        std.debug.print("   ID {d:>6}: ", .{id});
        
        var is_printable = true;
        for (token) |byte| {
            if (byte < 32 or byte > 126) {
                is_printable = false;
                break;
            }
        }
        
        if (is_printable and token.len > 0 and token.len < 50) {
            std.debug.print("\"{s}\"\n", .{token});
        } else {
            std.debug.print("[{d} bytes]\n", .{token.len});
        }
        
        count += 1;
    }
    std.debug.print("   ... and {d} more tokens\n", .{tokenizer.vocabSize() - 20});
    
    // Test token lookup
    std.debug.print("\nðŸ” Token Lookup Tests:\n", .{});
    
    const test_tokens = [_][]const u8{
        "the",
        "is",
        "a",
        "Hello",
        "world",
    };
    
    for (test_tokens) |token| {
        if (tokenizer.vocab.getId(token)) |id| {
            std.debug.print("   \"{s}\" -> ID {d}\n", .{ token, id });
        } else {
            std.debug.print("   \"{s}\" -> Not found\n", .{token});
        }
    }
    
    // Test reverse lookup
    std.debug.print("\nðŸ”„ Reverse Lookup Tests:\n", .{});
    
    const test_ids = [_]u32{ 0, 1, 2, 100, 1000, 10000 };
    
    for (test_ids) |id| {
        if (tokenizer.vocab.getToken(id)) |token| {
            if (token.len < 50) {
                std.debug.print("   ID {d} -> \"{s}\"\n", .{ id, token });
            } else {
                std.debug.print("   ID {d} -> [{d} bytes]\n", .{ id, token.len });
            }
        } else {
            std.debug.print("   ID {d} -> Not found\n", .{id});
        }
    }
    
    // Test BPE merges
    std.debug.print("\nðŸ”— Sample BPE Merges (first 10):\n", .{});
    var merge_it = tokenizer.merges.iterator();
    count = 0;
    while (merge_it.next()) |entry| {
        if (count >= 10) break;
        
        const merge_pair = entry.key_ptr.*;
        const rank = entry.value_ptr.*;
        
        std.debug.print("   Rank {d:>6}: \"{s}\"\n", .{ rank, merge_pair });
        count += 1;
    }
    std.debug.print("   ... and {d} more merges\n", .{tokenizer.merges.count() - 10});
    
    // Vocabulary coverage analysis
    std.debug.print("\nðŸ“ˆ Vocabulary Analysis:\n", .{});
    
    var ascii_count: usize = 0;
    var unicode_count: usize = 0;
    var special_count: usize = 0;
    
    var token_it = tokenizer.vocab.token_to_id.iterator();
    while (token_it.next()) |entry| {
        const token = entry.key_ptr.*;
        
        if (token.len > 0) {
            if (token[0] == '<' and token[token.len - 1] == '>') {
                special_count += 1;
            } else if (token.len == 1 and token[0] < 128) {
                ascii_count += 1;
            } else {
                unicode_count += 1;
            }
        }
    }
    
    std.debug.print("   ASCII tokens: {d} ({d:.1}%)\n", .{
        ascii_count,
        @as(f64, @floatFromInt(ascii_count)) * 100.0 / @as(f64, @floatFromInt(tokenizer.vocabSize())),
    });
    std.debug.print("   Unicode tokens: {d} ({d:.1}%)\n", .{
        unicode_count,
        @as(f64, @floatFromInt(unicode_count)) * 100.0 / @as(f64, @floatFromInt(tokenizer.vocabSize())),
    });
    std.debug.print("   Special tokens: {d} ({d:.1}%)\n", .{
        special_count,
        @as(f64, @floatFromInt(special_count)) * 100.0 / @as(f64, @floatFromInt(tokenizer.vocabSize())),
    });
    
    std.debug.print("\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    std.debug.print("âœ… ALL BPE TOKENIZER TESTS PASSED!\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    
    std.debug.print("\nðŸ“Š BPE Tokenizer Features:\n", .{});
    std.debug.print("   â€¢ Load vocab.json (151K+ tokens for Qwen3)\n", .{});
    std.debug.print("   â€¢ Load merges.txt (BPE rules)\n", .{});
    std.debug.print("   â€¢ Token â†” ID bidirectional lookup\n", .{});
    std.debug.print("   â€¢ Special token handling\n", .{});
    std.debug.print("   â€¢ Unicode support\n", .{});
    std.debug.print("   â€¢ Memory-efficient HashMaps\n", .{});
    std.debug.print("\nðŸš€ Ready for text encoding/decoding!\n", .{});
}
