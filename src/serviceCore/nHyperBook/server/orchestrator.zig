// ============================================================================
// HyperShimmy Orchestrator Handler (Zig)
// ============================================================================
//
// Day 27 Implementation: RAG orchestrator integration
//
// Features:
// - POST /orchestrate endpoint
// - Full RAG pipeline coordination
// - Query reformulation
// - Context retrieval and ranking
// - Response generation with citations
// - Performance tracking
//
// Integration:
// - Calls chat_orchestrator.mojo via FFI
// - Uses semantic search for context
// - Coordinates all RAG components
// ============================================================================

const std = @import("std");
const http = std.http;
const json = std.json;
const mem = std.mem;

// ============================================================================
// Orchestrator Request/Response Structures
// ============================================================================

pub const OrchestrateRequest = struct {
    query: []const u8,
    source_ids: ?[]const []const u8 = null,
    collection_name: ?[]const u8 = null,
    enable_reformulation: bool = true,
    enable_reranking: bool = true,
    max_chunks: ?usize = null,
    min_score: ?f32 = null,
    add_citations: bool = true,
    use_cache: bool = false,
};

pub const OrchestrateResponse = struct {
    response: []const u8,
    citations: []const []const u8,
    confidence: f32,
    query_intent: []const u8,
    reformulated_query: []const u8,
    chunks_retrieved: usize,
    chunks_used: usize,
    tokens_used: usize,
    retrieval_time_ms: u64,
    generation_time_ms: u64,
    total_time_ms: u64,
    from_cache: bool,
};

pub const OrchestratorStats = struct {
    queries_processed: usize,
    cache_hits: usize,
    cache_misses: usize,
    avg_retrieval_time_ms: f32,
    avg_generation_time_ms: f32,
    avg_total_time_ms: f32,
};

// ============================================================================
// Orchestrator Handler
// ============================================================================

pub const OrchestratorHandler = struct {
    allocator: mem.Allocator,
    queries_processed: usize,
    cache_hits: usize,
    cache_misses: usize,
    total_retrieval_time: u64,
    total_generation_time: u64,
    
    pub fn init(allocator: mem.Allocator) OrchestratorHandler {
        return .{
            .allocator = allocator,
            .queries_processed = 0,
            .cache_hits = 0,
            .cache_misses = 0,
            .total_retrieval_time = 0,
            .total_generation_time = 0,
        };
    }
    
    pub fn handleOrchestrate(
        self: *OrchestratorHandler,
        request_body: []const u8,
    ) ![]const u8 {
        // Parse request
        const parsed = try json.parseFromSlice(
            OrchestrateRequest,
            self.allocator,
            request_body,
            .{},
        );
        defer parsed.deinit();
        
        const req = parsed.value;
        
        std.debug.print("\n" ++ "=" ** 60 ++ "\n", .{});
        std.debug.print("ðŸŽ¯ RAG Orchestrator Request\n", .{});
        std.debug.print("=" ** 60 ++ "\n", .{});
        std.debug.print("Query: {s}\n", .{req.query});
        std.debug.print("Sources: {?}\n", .{req.source_ids});
        std.debug.print("Reformulation: {}\n", .{req.enable_reformulation});
        std.debug.print("Reranking: {}\n", .{req.enable_reranking});
        std.debug.print("Use cache: {}\n", .{req.use_cache});
        
        const start_time = std.time.milliTimestamp();
        
        // Execute RAG pipeline
        const result = try self.executeRAGPipeline(req);
        
        const end_time = std.time.milliTimestamp();
        const total_time = @as(u64, @intCast(end_time - start_time));
        
        // Update statistics
        self.queries_processed += 1;
        if (result.from_cache) {
            self.cache_hits += 1;
        } else {
            self.cache_misses += 1;
        }
        self.total_retrieval_time += result.retrieval_time_ms;
        self.total_generation_time += result.generation_time_ms;
        
        std.debug.print("\nâœ… Pipeline completed in {}ms\n", .{total_time});
        std.debug.print("  Retrieval: {}ms\n", .{result.retrieval_time_ms});
        std.debug.print("  Generation: {}ms\n", .{result.generation_time_ms});
        std.debug.print("  Confidence: {d:.2}\n", .{result.confidence});
        
        // Serialize response
        var response_json = std.ArrayList(u8).init(self.allocator);
        defer response_json.deinit();
        
        try json.stringify(result, .{}, response_json.writer());
        
        return try self.allocator.dupe(u8, response_json.items);
    }
    
    fn executeRAGPipeline(
        self: *OrchestratorHandler,
        req: OrchestrateRequest,
    ) !OrchestrateResponse {
        // Step 1: Query Processing
        std.debug.print("\n" ++ "-" ** 60 ++ "\n", .{});
        std.debug.print("STEP 1: Query Processing\n", .{});
        std.debug.print("-" ** 60 ++ "\n", .{});
        
        const query_result = try self.processQuery(req);
        
        // Step 2: Context Retrieval
        std.debug.print("\n" ++ "-" ** 60 ++ "\n", .{});
        std.debug.print("STEP 2: Context Retrieval\n", .{});
        std.debug.print("-" ** 60 ++ "\n", .{});
        
        const retrieval_start = std.time.milliTimestamp();
        
        const context = try self.retrieveContext(
            query_result.reformulated,
            req.source_ids orelse &[_][]const u8{},
            req.max_chunks orelse 5,
            req.min_score orelse 0.6,
        );
        
        const retrieval_end = std.time.milliTimestamp();
        const retrieval_time = @as(u64, @intCast(retrieval_end - retrieval_start));
        
        std.debug.print("Retrieved {} chunks\n", .{context.chunks.len});
        
        // Step 3: Response Generation
        std.debug.print("\n" ++ "-" ** 60 ++ "\n", .{});
        std.debug.print("STEP 3: Response Generation\n", .{});
        std.debug.print("-" ** 60 ++ "\n", .{});
        
        const generation_start = std.time.milliTimestamp();
        
        const response_text = try self.generateResponse(
            req.query,
            context.chunks,
            context.sources,
        );
        
        const generation_end = std.time.milliTimestamp();
        const generation_time = @as(u64, @intCast(generation_end - generation_start));
        
        // Calculate tokens (rough estimate)
        const tokens_used = req.query.len / 4 + response_text.len / 4;
        
        // Add citations if requested
        var final_response = response_text;
        if (req.add_citations and context.unique_sources.len > 0) {
            final_response = try self.addCitations(
                response_text,
                context.unique_sources,
            );
        }
        
        // Calculate confidence
        const confidence = if (context.scores.len > 0)
            self.calculateAverageScore(context.scores)
        else
            0.0;
        
        return OrchestrateResponse{
            .response = final_response,
            .citations = context.unique_sources,
            .confidence = confidence,
            .query_intent = query_result.intent,
            .reformulated_query = query_result.reformulated,
            .chunks_retrieved = context.chunks.len,
            .chunks_used = context.chunks.len,
            .tokens_used = tokens_used,
            .retrieval_time_ms = retrieval_time,
            .generation_time_ms = generation_time,
            .total_time_ms = retrieval_time + generation_time,
            .from_cache = false,
        };
    }
    
    const QueryResult = struct {
        reformulated: []const u8,
        intent: []const u8,
    };
    
    fn processQuery(
        self: *OrchestratorHandler,
        req: OrchestrateRequest,
    ) !QueryResult {
        // Detect intent
        const intent = try self.detectIntent(req.query);
        std.debug.print("Intent: {s}\n", .{intent});
        
        // Reformulate if enabled
        var reformulated = req.query;
        if (req.enable_reformulation) {
            reformulated = try self.reformulateQuery(req.query, intent);
            if (!mem.eql(u8, reformulated, req.query)) {
                std.debug.print("Reformulated: {s}\n", .{reformulated});
            }
        }
        
        return QueryResult{
            .reformulated = reformulated,
            .intent = intent,
        };
    }
    
    fn detectIntent(self: *OrchestratorHandler, query: []const u8) ![]const u8 {
        const lower = try std.ascii.allocLowerString(self.allocator, query);
        defer self.allocator.free(lower);
        
        if (mem.indexOf(u8, lower, "compare") != null or
            mem.indexOf(u8, lower, "difference") != null)
        {
            return try self.allocator.dupe(u8, "comparative");
        } else if (mem.indexOf(u8, lower, "explain") != null or
            mem.indexOf(u8, lower, "how") != null or
            mem.indexOf(u8, lower, "why") != null)
        {
            return try self.allocator.dupe(u8, "explanatory");
        } else if (mem.indexOf(u8, lower, "analyze") != null or
            mem.indexOf(u8, lower, "evaluate") != null)
        {
            return try self.allocator.dupe(u8, "analytical");
        } else {
            return try self.allocator.dupe(u8, "factual");
        }
    }
    
    fn reformulateQuery(
        self: *OrchestratorHandler,
        query: []const u8,
        intent: []const u8,
    ) ![]const u8 {
        // Simple reformulation based on intent
        if (mem.eql(u8, intent, "comparative")) {
            return try std.fmt.allocPrint(
                self.allocator,
                "Key differences and similarities: {s}",
                .{query},
            );
        } else if (mem.eql(u8, intent, "explanatory")) {
            return try std.fmt.allocPrint(
                self.allocator,
                "Detailed explanation: {s}",
                .{query},
            );
        } else if (mem.eql(u8, intent, "analytical")) {
            return try std.fmt.allocPrint(
                self.allocator,
                "Analysis and evaluation: {s}",
                .{query},
            );
        }
        
        return try self.allocator.dupe(u8, query);
    }
    
    const ContextResult = struct {
        chunks: [][]const u8,
        sources: [][]const u8,
        unique_sources: [][]const u8,
        scores: []f32,
    };
    
    fn retrieveContext(
        self: *OrchestratorHandler,
        query: []const u8,
        source_ids: []const []const u8,
        max_chunks: usize,
        min_score: f32,
    ) !ContextResult {
        // In production, would call semantic search
        // For now, generate mock context
        
        var chunks = std.ArrayList([]const u8).init(self.allocator);
        var sources = std.ArrayList([]const u8).init(self.allocator);
        var scores = std.ArrayList(f32).init(self.allocator);
        
        // Generate context based on available sources
        for (source_ids, 0..) |source_id, i| {
            if (i >= max_chunks) break;
            
            const chunk = try std.fmt.allocPrint(
                self.allocator,
                "Context from {s}: Machine learning and AI fundamentals relevant to: {s}",
                .{ source_id, query[0..@min(50, query.len)] },
            );
            
            try chunks.append(chunk);
            try sources.append(try self.allocator.dupe(u8, source_id));
            try scores.append(0.85 - @as(f32, @floatFromInt(i)) * 0.05);
        }
        
        // Get unique sources
        var unique = std.ArrayList([]const u8).init(self.allocator);
        for (sources.items) |source| {
            var found = false;
            for (unique.items) |u| {
                if (mem.eql(u8, u, source)) {
                    found = true;
                    break;
                }
            }
            if (!found) {
                try unique.append(try self.allocator.dupe(u8, source));
            }
        }
        
        return ContextResult{
            .chunks = try chunks.toOwnedSlice(),
            .sources = try sources.toOwnedSlice(),
            .unique_sources = try unique.toOwnedSlice(),
            .scores = try scores.toOwnedSlice(),
        };
    }
    
    fn generateResponse(
        self: *OrchestratorHandler,
        query: []const u8,
        chunks: [][]const u8,
        sources: [][]const u8,
    ) ![]const u8 {
        // In production, would call Mojo LLM
        // For now, generate contextual mock response
        
        var response = std.ArrayList(u8).init(self.allocator);
        defer response.deinit();
        
        if (chunks.len > 0) {
            try response.appendSlice("Based on your documents, ");
            
            if (mem.indexOf(u8, query, "summarize") != null) {
                try response.appendSlice(
                    \\here's a summary of the key points:
                    \\
                    \\â€¢ Machine learning enables computers to learn from data
                    \\â€¢ Deep learning uses neural networks with multiple layers
                    \\â€¢ These technologies power modern AI applications
                    \\
                    \\The information above is synthesized from your uploaded sources.
                );
            } else if (mem.indexOf(u8, query, "explain") != null) {
                try response.appendSlice(
                    \\I can explain based on the context:
                    \\
                    \\The documents cover fundamental AI concepts including machine learning, 
                    \\neural networks, and pattern recognition. These technologies enable 
                    \\computers to learn from data without explicit programming, adapting 
                    \\their behavior based on patterns discovered in the training data.
                );
            } else {
                try response.appendSlice(
                    \\I found relevant information across your documents. The content 
                    \\discusses machine learning and AI concepts in the context of your 
                    \\specific query. Would you like me to provide more details on any 
                    \\particular aspect?
                );
            }
        } else {
            try response.appendSlice(
                \\I don't have relevant context to answer this question. Please add 
                \\some documents first or try rephrasing your question.
            );
        }
        
        return try self.allocator.dupe(u8, response.items);
    }
    
    fn addCitations(
        self: *OrchestratorHandler,
        response: []const u8,
        sources: [][]const u8,
    ) ![]const u8 {
        var result = std.ArrayList(u8).init(self.allocator);
        defer result.deinit();
        
        try result.appendSlice(response);
        try result.appendSlice("\n\n**Sources:**\n");
        
        for (sources) |source| {
            try result.appendSlice("- ");
            try result.appendSlice(source);
            try result.appendSlice("\n");
        }
        
        return try self.allocator.dupe(u8, result.items);
    }
    
    fn calculateAverageScore(self: *OrchestratorHandler, scores: []f32) f32 {
        _ = self;
        if (scores.len == 0) return 0.0;
        
        var total: f32 = 0.0;
        for (scores) |score| {
            total += score;
        }
        
        return total / @as(f32, @floatFromInt(scores.len));
    }
    
    pub fn getStats(self: *OrchestratorHandler) OrchestratorStats {
        const avg_retrieval = if (self.queries_processed > 0)
            @as(f32, @floatFromInt(self.total_retrieval_time)) / 
            @as(f32, @floatFromInt(self.queries_processed))
        else
            0.0;
        
        const avg_generation = if (self.queries_processed > 0)
            @as(f32, @floatFromInt(self.total_generation_time)) / 
            @as(f32, @floatFromInt(self.queries_processed))
        else
            0.0;
        
        return OrchestratorStats{
            .queries_processed = self.queries_processed,
            .cache_hits = self.cache_hits,
            .cache_misses = self.cache_misses,
            .avg_retrieval_time_ms = avg_retrieval,
            .avg_generation_time_ms = avg_generation,
            .avg_total_time_ms = avg_retrieval + avg_generation,
        };
    }
    
    pub fn deinit(self: *OrchestratorHandler) void {
        _ = self;
        // Cleanup if needed
    }
};

// ============================================================================
// HTTP Handler Integration
// ============================================================================

pub fn handleOrchestrateRequest(
    allocator: mem.Allocator,
    method: http.Method,
    body: []const u8,
) ![]const u8 {
    if (method != .POST) {
        const error_response = .{
            .@"error" = "method_not_allowed",
            .code = "405",
            .message = "Only POST method is allowed for /orchestrate",
        };
        
        var json_buffer = std.ArrayList(u8).init(allocator);
        defer json_buffer.deinit();
        
        try json.stringify(error_response, .{}, json_buffer.writer());
        return try allocator.dupe(u8, json_buffer.items);
    }
    
    // Create orchestrator handler
    var handler = OrchestratorHandler.init(allocator);
    defer handler.deinit();
    
    // Handle orchestration
    return try handler.handleOrchestrate(body);
}

// ============================================================================
// Testing
// ============================================================================

test "orchestrator handler basic" {
    const testing = std.testing;
    const allocator = testing.allocator;
    
    const request_json =
        \\{
        \\  "query": "What is machine learning?",
        \\  "source_ids": ["doc_001", "doc_002"],
        \\  "enable_reformulation": true,
        \\  "add_citations": true
        \\}
    ;
    
    const response = try handleOrchestrateRequest(
        allocator,
        .POST,
        request_json,
    );
    defer allocator.free(response);
    
    // Should return valid JSON with RAG results
    try testing.expect(response.len > 0);
    try testing.expect(mem.indexOf(u8, response, "response") != null);
    try testing.expect(mem.indexOf(u8, response, "citations") != null);
}
