# tensor_helper.mojo
# Migrated from tensor_helper.py - Pure Mojo Implementation
# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# -----------------------------------------------------------------------------

from collections import Dict, List
from memory import memset_zero, memcpy
from algorithm import vectorize
from sys.info import simdwidthof

# Native Mojo tensor implementation using SIMD and memory management
# This replaces torch tensor operations with pure Mojo

alias TensorShape = List[Int]
alias TensorData = DTypePointer[DType.int32]


struct Tensor:
    """
    Native Mojo tensor for LLM token operations.
    Optimized for CPU with SIMD vectorization.
    """
    var data: TensorData
    var shape: TensorShape
    var size: Int
    
    fn __init__(inout self, shape: TensorShape) raises:
        """Initialize tensor with given shape"""
        self.shape = shape
        self.size = 1
        for dim in self.shape:
            self.size *= dim[]
        
        self.data = TensorData.alloc(self.size)
        memset_zero(self.data, self.size)
    
    fn __init__(inout self, data: List[Int], shape: TensorShape) raises:
        """Initialize tensor from data and shape"""
        self.shape = shape
        self.size = 1
        for dim in self.shape:
            self.size *= dim[]
        
        if len(data) != self.size:
            raise Error("Data size does not match tensor size")
        
        self.data = TensorData.alloc(self.size)
        for i in range(len(data)):
            self.data[i] = data[i]
    
    fn __del__(owned self):
        """Free tensor memory"""
        self.data.free()
    
    fn get(self, idx: Int) -> Int:
        """Get value at index"""
        return int(self.data[idx])
    
    fn set(inout self, idx: Int, value: Int):
        """Set value at index"""
        self.data[idx] = value
    
    fn fill(inout self, value: Int):
        """Fill tensor with value"""
        @parameter
        fn vectorized_fill[simd_width: Int](idx: Int):
            self.data.store[width=simd_width](idx, value)
        
        vectorize[vectorized_fill, simdwidthof[DType.int32]()](self.size)
    
    fn copy(self) raises -> Tensor:
        """Create a copy of this tensor"""
        var result = Tensor(self.shape)
        memcpy(result.data, self.data, self.size)
        return result
    
    fn get_row(self, row_idx: Int) raises -> List[Int]:
        """Get a row from 2D tensor"""
        if len(self.shape) != 2:
            raise Error("get_row requires 2D tensor")
        
        let cols = self.shape[1]
        let offset = row_idx * cols
        var result = List[Int]()
        
        for i in range(cols):
            result.append(int(self.data[offset + i]))
        
        return result
    
    fn slice(self, start: Int, end: Int) raises -> Tensor:
        """Slice tensor along first dimension"""
        if len(self.shape) == 1:
            var new_shape = TensorShape()
            new_shape.append(end - start)
            var result = Tensor(new_shape)
            
            for i in range(start, end):
                result.data[i - start] = self.data[i]
            
            return result
        else:
            # For 2D tensor, slice rows
            let cols = self.shape[1]
            var new_shape = TensorShape()
            new_shape.append(end - start)
            new_shape.append(cols)
            var result = Tensor(new_shape)
            
            let src_offset = start * cols
            let copy_size = (end - start) * cols
            memcpy(result.data, self.data.offset(src_offset), copy_size)
            
            return result


struct TensorConfig:
    """Configuration for tensor operations"""
    var pad_token_id: Int
    var max_prompt_length: Int
    
    fn __init__(inout self, pad_token_id: Int, max_prompt_length: Int):
        self.pad_token_id = pad_token_id
        self.max_prompt_length = max_prompt_length


struct TensorDict:
    """Dictionary of tensors for batch processing"""
    var tensors: Dict[String, Tensor]
    
    fn __init__(inout self):
        self.tensors = Dict[String, Tensor]()
    
    fn __setitem__(inout self, key: String, value: Tensor):
        self.tensors[key] = value
    
    fn __getitem__(self, key: String) raises -> Tensor:
        return self.tensors[key]
    
    fn contains(self, key: String) -> Bool:
        return key in self.tensors


struct TensorHelper:
    """
    Helper class for tensor operations in LLM generation.
    Handles padding, masking, and sequence manipulations using native Mojo.
    """
    var config: TensorConfig
    
    fn __init__(inout self, config: TensorConfig):
        self.config = config
    
    fn create_attention_mask(self, input_ids: Tensor) raises -> Tensor:
        """
        Create attention mask from input ids.
        Mask is 1 for non-pad tokens, 0 for pad tokens.
        """
        var mask = input_ids.copy()
        
        for i in range(mask.size):
            if mask.data[i] == self.config.pad_token_id:
                mask.data[i] = 0
            else:
                mask.data[i] = 1
        
        return mask
    
    fn create_position_ids(self, attention_mask: Tensor) raises -> Tensor:
        """
        Create position ids from attention mask.
        Position IDs are cumulative sum of attention mask along sequence dimension.
        """
        var position_ids = attention_mask.copy()
        
        if len(attention_mask.shape) == 1:
            # 1D case
            var cumsum = 0
            for i in range(attention_mask.size):
                if attention_mask.data[i] == 1:
                    position_ids.data[i] = cumsum
                    cumsum += 1
                else:
                    position_ids.data[i] = 0
        else:
            # 2D case (batch_size, seq_len)
            let batch_size = attention_mask.shape[0]
            let seq_len = attention_mask.shape[1]
            
            for b in range(batch_size):
                var cumsum = 0
                for s in range(seq_len):
                    let idx = b * seq_len + s
                    if attention_mask.data[idx] == 1:
                        position_ids.data[idx] = cumsum
                        cumsum += 1
                    else:
                        position_ids.data[idx] = 0
        
        return position_ids
    
    fn get_effective_length(self, tensor: Tensor) raises -> Int:
        """
        Get effective length of tensor (number of non-pad tokens).
        """
        var length = 0
        for i in range(tensor.size):
            if tensor.data[i] != self.config.pad_token_id:
                length += 1
        
        return length
    
    fn cut_to_effective_len(
        self, 
        tensor_dict: TensorDict, 
        keys: List[String],
        cut_left: Bool = True
    ) raises -> TensorDict:
        """
        Cut tensors to their effective length based on attention mask.
        If cut_left=True, remove padding from left, otherwise from right.
        """
        var result = TensorDict()
        
        # Get attention mask to determine effective length
        let attention_mask = tensor_dict["attention_mask"]
        
        if len(attention_mask.shape) == 1:
            # Find first and last non-zero positions
            var first_nonzero = -1
            var last_nonzero = -1
            
            for i in range(attention_mask.size):
                if attention_mask.data[i] != 0:
                    if first_nonzero == -1:
                        first_nonzero = i
                    last_nonzero = i
            
            if first_nonzero == -1:
                # All padding, return empty
                return result
            
            # Slice all tensors
            for key in keys:
                let tensor = tensor_dict[key[]]
                if cut_left:
                    result[key[]] = tensor.slice(first_nonzero, last_nonzero + 1)
                else:
                    result[key[]] = tensor.slice(0, last_nonzero + 1)
        else:
            # 2D case - process each batch element
            # For simplicity, just copy all for now
            # Full implementation would handle batching properly
            for key in keys:
                result[key[]] = tensor_dict[key[]]
        
        return result
    
    fn convert_pad_structure(
        self,
        tensor: Tensor,
        pad_to_left: Bool = True
    ) raises -> Tensor:
        """
        Convert padding structure of tensor.
        If pad_to_left=True, move padding to left side.
        Otherwise, move padding to right side.
        """
        if len(tensor.shape) != 1:
            # For 2D, process each row
            # Simplified implementation
            return tensor.copy()
        
        # Collect non-pad tokens
        var non_pad = List[Int]()
        var pad_count = 0
        
        for i in range(tensor.size):
            if tensor.data[i] == self.config.pad_token_id:
                pad_count += 1
            else:
                non_pad.append(int(tensor.data[i]))
        
        # Create result tensor
        var result = tensor.copy()
        
        if pad_to_left:
            # Padding on left, tokens on right
            for i in range(pad_count):
                result.data[i] = self.config.pad_token_id
            for i in range(len(non_pad)):
                result.data[pad_count + i] = non_pad[i]
        else:
            # Tokens on left, padding on right
            for i in range(len(non_pad)):
                result.data[i] = non_pad[i]
            for i in range(pad_count):
                result.data[len(non_pad) + i] = self.config.pad_token_id
        
        return result
    
    fn pad_to_max_length(
        self,
        tensor: Tensor,
        max_length: Int,
        pad_to_left: Bool = True
    ) raises -> Tensor:
        """
        Pad tensor to max_length.
        """
        let current_length = tensor.shape[0] if len(tensor.shape) == 1 else tensor.shape[1]
        
        if current_length >= max_length:
            return tensor.copy()
        
        let pad_size = max_length - current_length
        
        var new_shape = TensorShape()
        new_shape.append(max_length)
        var result = Tensor(new_shape)
        result.fill(self.config.pad_token_id)
        
        if pad_to_left:
            # Copy data to right side
            for i in range(current_length):
                result.data[pad_size + i] = tensor.data[i]
        else:
            # Copy data to left side
            for i in range(current_length):
                result.data[i] = tensor.data[i]
        
        return result
    
    fn concatenate_tensors(
        self,
        tensors: List[Tensor],
        pad_to_left: Bool = True
    ) raises -> Tensor:
        """
        Concatenate multiple tensors with padding.
        """
        if len(tensors) == 0:
            raise Error("Cannot concatenate empty tensor list")
        
        # Find max length
        var max_len = 0
        for t in tensors:
            let t_len = t[].shape[0] if len(t[].shape) == 1 else t[].shape[1]
            if t_len > max_len:
                max_len = t_len
        
        # Pad all tensors to max length
        var padded = List[Tensor]()
        for t in tensors:
            padded.append(self.pad_to_max_length(t[], max_len, pad_to_left))
        
        # Create result tensor (batch_size, max_len)
        var new_shape = TensorShape()
        new_shape.append(len(tensors))
        new_shape.append(max_len)
        var result = Tensor(new_shape)
        
        # Copy data
        for i in range(len(padded)):
            for j in range(max_len):
                result.data[i * max_len + j] = padded[i].data[j]
        
        return result


fn main() raises:
    """Entry point for tensor helper module"""
    print("=" * 80)
    print("TensorHelper - Pure Mojo Implementation")
    print("=" * 80)
    print("")
    print("Features:")
    print("  - Native Mojo tensor operations (no PyTorch dependency)")
    print("  - SIMD-optimized operations")
    print("  - Attention mask creation")
    print("  - Position ID generation")
    print("  - Padding and masking utilities")
    print("  - Sequence manipulation")
    print("")
    
    # Test basic tensor operations
    print("Testing Tensor Operations:")
    print("-" * 40)
    
    # Create a simple tensor
    var shape = TensorShape()
    shape.append(5)
    var tensor = Tensor(shape)
    
    # Fill with test data
    tensor.set(0, 1)
    tensor.set(1, 2)
    tensor.set(2, 0)  # pad token
    tensor.set(3, 3)
    tensor.set(4, 0)  # pad token
    
    print("Original tensor: [1, 2, 0, 3, 0]")
    
    # Create config and helper
    let config = TensorConfig(pad_token_id=0, max_prompt_length=128)
    let helper = TensorHelper(config)
    
    # Test attention mask creation
    let mask = helper.create_attention_mask(tensor)
    print("Attention mask: ", end="")
    for i in range(mask.size):
        print(mask.get(i), end=" ")
    print("")
    
    # Test position IDs
    let pos_ids = helper.create_position_ids(mask)
    print("Position IDs: ", end="")
    for i in range(pos_ids.size):
        print(pos_ids.get(i), end=" ")
    print("")
    
    # Test effective length
    let eff_len = helper.get_effective_length(tensor)
    print("Effective length:", eff_len)
    
    print("")
    print("=" * 80)
    print("âœ… Pure Mojo tensor operations working!")
    print("=" * 80)
