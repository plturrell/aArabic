const std = @import("std");
const sharded = @import("safetensors_sharded");

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();
    
    std.debug.print("\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    std.debug.print("  SHARDED SAFETENSORS LOADER TESTS\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    
    // Test with Qwen3 model (16 shards)
    const base_path = "vendor/layerModels/huggingFace/Qwen/Qwen3-Coder-30B-A3B-Instruct";
    const index_file = "model.safetensors.index.json";
    
    std.debug.print("\nğŸ§ª Testing with Qwen3-Coder-30B (16 shards)\n", .{});
    std.debug.print("   Base path: {s}\n", .{base_path});
    std.debug.print("   Index: {s}\n", .{index_file});
    
    var loader = sharded.SafeTensorsSharded.init(allocator, base_path);
    defer loader.deinit();
    
    // Construct full index path
    const index_path = try std.fs.path.join(
        allocator,
        &[_][]const u8{ base_path, index_file },
    );
    defer allocator.free(index_path);
    
    // Load the model
    try loader.loadFromIndex(index_path);
    
    // Show summary
    loader.listTensors();
    
    // Test tensor lookup
    std.debug.print("\nğŸ§ª Testing tensor lookup and loading...\n", .{});
    
    // Try to load a common tensor name
    const test_tensor_names = [_][]const u8{
        "model.embed_tokens.weight",
        "model.layers.0.self_attn.q_proj.weight",
        "model.layers.0.self_attn.k_proj.weight",
        "lm_head.weight",
    };
    
    for (test_tensor_names) |tensor_name| {
        if (loader.hasTensor(tensor_name)) {
            std.debug.print("\n   Found tensor: {s}\n", .{tensor_name});
            
            // Get tensor info
            if (loader.getTensorInfo(tensor_name)) |info| {
                std.debug.print("     Shape: [", .{});
                for (info.shape, 0..) |dim, i| {
                    if (i > 0) std.debug.print(", ", .{});
                    std.debug.print("{d}", .{dim});
                }
                std.debug.print("]\n", .{});
                std.debug.print("     Dtype: {s}\n", .{@tagName(info.dtype)});
                std.debug.print("     Elements: {d}\n", .{info.elementCount()});
                
                // Load first 100 elements
                const tensor_data = try loader.getTensor(tensor_name);
                defer allocator.free(tensor_data);
                
                std.debug.print("     Loaded {d} elements\n", .{tensor_data.len});
                
                // Show first few values
                std.debug.print("     First 5 values: ", .{});
                const show_count = @min(5, tensor_data.len);
                for (tensor_data[0..show_count], 0..) |val, i| {
                    if (i > 0) std.debug.print(", ", .{});
                    std.debug.print("{d:.6}", .{val});
                }
                std.debug.print("\n", .{});
                
                // Calculate statistics
                var sum: f32 = 0.0;
                var min_val: f32 = tensor_data[0];
                var max_val: f32 = tensor_data[0];
                
                for (tensor_data) |val| {
                    sum += val;
                    min_val = @min(min_val, val);
                    max_val = @max(max_val, val);
                }
                
                const mean = sum / @as(f32, @floatFromInt(tensor_data.len));
                
                std.debug.print("     Statistics:\n", .{});
                std.debug.print("       Mean: {d:.6}\n", .{mean});
                std.debug.print("       Min:  {d:.6}\n", .{min_val});
                std.debug.print("       Max:  {d:.6}\n", .{max_val});
                std.debug.print("     âœ… Tensor loaded successfully!\n", .{});
            } else |err| {
                std.debug.print("     âŒ Error getting tensor info: {}\n", .{err});
            }
        } else {
            std.debug.print("\n   Tensor not found: {s}\n", .{tensor_name});
        }
    }
    
    // Show detailed list of first few tensors
    std.debug.print("\nğŸ“‹ Sample tensors (first 20):\n", .{});
    loader.listTensorsDetailed();
    
    std.debug.print("\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    std.debug.print("âœ… ALL SHARDED LOADER TESTS PASSED!\n", .{});
    std.debug.print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", .{});
    std.debug.print("\nğŸ“Š Multi-Shard Loader Features:\n", .{});
    std.debug.print("   â€¢ Parse model.safetensors.index.json\n", .{});
    std.debug.print("   â€¢ Load all shards in parallel-ready structure\n", .{});
    std.debug.print("   â€¢ Map tensor names to correct shard files\n", .{});
    std.debug.print("   â€¢ Efficient tensor lookup across shards\n", .{});
    std.debug.print("   â€¢ Support for 16+ shard models (tested with Qwen3-30B)\n", .{});
}
