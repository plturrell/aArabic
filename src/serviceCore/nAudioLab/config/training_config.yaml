# FastSpeech2 Training Configuration
# AudioLabShimmy - Day 19+
# 8-day training plan (200k steps)

dataset:
  name: "LJSpeech-1.1"
  manifest_path: "data/datasets/ljspeech_processed/training_manifest.json"
  sample_rate: 48000
  n_mels: 128
  train_split: 0.95  # 12,445 samples
  val_split: 0.05    # 655 samples
  random_seed: 42

model:
  # Encoder
  encoder_layers: 4
  encoder_hidden: 256
  encoder_heads: 4
  encoder_ff_dim: 1024
  
  # Decoder
  decoder_layers: 4
  decoder_hidden: 256
  decoder_heads: 4
  decoder_ff_dim: 1024
  
  # Variance Adaptors
  duration_predictor_layers: 2
  duration_predictor_channels: 256
  pitch_predictor_layers: 2
  pitch_predictor_channels: 256
  energy_predictor_layers: 2
  energy_predictor_channels: 256
  
  # Other
  dropout: 0.1
  max_seq_len: 1000
  phoneme_vocab_size: 70

training:
  # Main settings
  max_steps: 200000
  batch_size: 16          # CPU-friendly for M3 Max
  gradient_accumulation: 2  # Effective batch size: 32
  
  # Optimization
  learning_rate: 1.0e-4
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_eps: 1.0e-8
  weight_decay: 1.0e-6
  grad_clip_norm: 1.0
  
  # Learning rate schedule
  warmup_steps: 4000
  scheduler: "noam"  # Transformer learning rate schedule
  
  # Loss weights
  mel_loss_weight: 1.0
  duration_loss_weight: 0.1
  pitch_loss_weight: 0.1
  energy_loss_weight: 0.1
  
  # Checkpointing
  save_every: 5000       # Save every 5k steps
  validate_every: 1000   # Validate every 1k steps
  log_every: 100         # Log metrics every 100 steps
  
  # Validation
  num_val_samples: 10    # Generate samples during validation

optimization:
  # CPU optimization
  use_accelerate: true
  num_threads: 16        # For M3 Max (16-core)
  mixed_precision: true  # FP16/FP32 mixed precision
  pin_memory: true
  prefetch_factor: 2
  
  # Data loading
  num_workers: 7         # CPU cores for data loading
  persistent_workers: true

paths:
  # Output directories
  checkpoint_dir: "data/models/fastspeech2/checkpoints"
  log_dir: "data/models/fastspeech2/logs"
  sample_dir: "data/models/fastspeech2/samples"
  tensorboard_dir: "data/models/fastspeech2/tensorboard"

# Day-by-day training plan (8 days)
training_schedule:
  day_19:
    steps: [0, 25000]
    expected_duration_hours: 24
    checkpoints: [5000, 10000, 15000, 20000, 25000]
  
  day_20:
    steps: [25000, 50000]
    expected_duration_hours: 24
    checkpoints: [30000, 35000, 40000, 45000, 50000]
  
  day_21:
    steps: [50000, 75000]
    expected_duration_hours: 24
    checkpoints: [55000, 60000, 65000, 70000, 75000]
  
  day_22:
    steps: [75000, 100000]
    expected_duration_hours: 24
    checkpoints: [80000, 85000, 90000, 95000, 100000]
  
  day_23:
    steps: [100000, 125000]
    expected_duration_hours: 24
    checkpoints: [105000, 110000, 115000, 120000, 125000]
  
  day_24:
    steps: [125000, 150000]
    expected_duration_hours: 24
    checkpoints: [130000, 135000, 140000, 145000, 150000]
  
  day_25:
    steps: [150000, 175000]
    expected_duration_hours: 24
    checkpoints: [155000, 160000, 165000, 170000, 175000]
  
  day_26:
    steps: [175000, 200000]
    expected_duration_hours: 24
    checkpoints: [180000, 185000, 190000, 195000, 200000]

# Validation sentences for monitoring
validation_sentences:
  - "Hello world, this is a test of the text to speech system."
  - "The quick brown fox jumps over the lazy dog."
  - "Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts."
  - "How are you doing today?"
  - "Neural networks are powerful machine learning models."
