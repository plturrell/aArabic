{
  "models": [
    {
      "name": "google-gemma-3-270m-it",
      "description": "Google Gemma 3 270M Instruction-Tuned (for testing)",
      "size": "540 MB",
      "format": "safetensors",
      "source": "huggingface",
      "hf_repo": "google/gemma-3-270m-it",
      "recommended_for": "testing, development",
      "gpu_memory": "2GB",
      "orchestration_categories": [
        "code"
      ],
      "agent_types": [
        "inference"
      ],
      "benchmarks": {},
      "hf_metadata": {
        "downloads": 112422,
        "likes": 533,
        "license": "",
        "languages": [],
        "datasets": [],
        "pipeline_tag": "text-generation",
        "tags": [
          "transformers",
          "safetensors",
          "gemma3_text",
          "text-generation",
          "gemma3",
          "gemma",
          "google",
          "conversational",
          "arxiv:2503.19786",
          "arxiv:1905.07830",
          "arxiv:1905.10044",
          "arxiv:1911.11641",
          "arxiv:1705.03551",
          "arxiv:1911.01547",
          "arxiv:1907.10641",
          "arxiv:2311.07911",
          "arxiv:2311.12022",
          "arxiv:2411.04368",
          "arxiv:1904.09728",
          "arxiv:1903.00161",
          "arxiv:2009.03300",
          "arxiv:2304.06364",
          "arxiv:2103.03874",
          "arxiv:2110.14168",
          "arxiv:2108.07732",
          "arxiv:2107.03374",
          "arxiv:2403.07974",
          "arxiv:2305.03111",
          "arxiv:2405.04520",
          "arxiv:2210.03057",
          "arxiv:2106.03193",
          "arxiv:1910.11856",
          "arxiv:2502.12404",
          "arxiv:2502.21228",
          "arxiv:2404.16816",
          "arxiv:2104.12756",
          "arxiv:2311.16502",
          "arxiv:2203.10244",
          "arxiv:2404.12390",
          "arxiv:1810.12440",
          "arxiv:1908.02660",
          "arxiv:2310.02255",
          "arxiv:2312.11805",
          "base_model:google/gemma-3-270m",
          "base_model:finetune:google/gemma-3-270m",
          "license:gemma",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "model_id": "google/gemma-3-270m-it",
        "author": "google",
        "last_modified": "2025-08-14T07:35:07.000Z"
      }
    },
    {
      "name": "LFM2.5-1.2B-Instruct-GGUF",
      "description": "Liquid Foundation Model 2.5 1.2B Instruction-Tuned",
      "size": "1.2 GB (Q4), 2.4 GB (F16)",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "LiquidAI/LFM-2.5-1.2B-Instruct-GGUF",
      "recommended_for": "production, inference",
      "gpu_memory": "4GB",
      "variants": [
        "Q4_0",
        "Q4_K_M",
        "F16"
      ],
      "agent_types": [
        "inference"
      ]
    },
    {
      "name": "HY-MT1.5-7B",
      "description": "Arabic Machine Translation Model 7B",
      "size": "4.2 GB (Q4), 7.6 GB (Q8)",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "Helsinki-NLP/opus-mt-ar-en",
      "recommended_for": "arabic translation",
      "gpu_memory": "8GB",
      "variants": [
        "Q4_K_M",
        "Q6_K",
        "Q8_0"
      ],
      "orchestration_categories": [
        "relational"
      ],
      "agent_types": [
        "inference",
        "tool"
      ],
      "benchmarks": {},
      "hf_metadata": {
        "downloads": 117979,
        "likes": 48,
        "license": "",
        "languages": [],
        "datasets": [],
        "pipeline_tag": "translation",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "rust",
          "marian",
          "text2text-generation",
          "translation",
          "ar",
          "en",
          "license:apache-2.0",
          "endpoints_compatible",
          "deploy:azure",
          "region:us"
        ],
        "model_id": "Helsinki-NLP/opus-mt-ar-en",
        "author": "Helsinki-NLP",
        "last_modified": "2023-08-16T11:25:35.000Z"
      },
      "training": {
        "datasets_description": "opus"
      }
    },
    {
      "name": "microsoft-phi-2",
      "description": "Microsoft Phi-2 2.7B",
      "size": "5.2 GB",
      "format": "safetensors",
      "source": "huggingface",
      "hf_repo": "microsoft/phi-2",
      "recommended_for": "general purpose",
      "gpu_memory": "6GB",
      "orchestration_categories": [
        "code"
      ],
      "agent_types": [
        "inference"
      ],
      "benchmarks": {},
      "hf_metadata": {
        "downloads": 1312936,
        "likes": 3421,
        "license": "",
        "languages": [],
        "datasets": [],
        "pipeline_tag": "text-generation",
        "tags": [
          "transformers",
          "safetensors",
          "phi",
          "text-generation",
          "nlp",
          "code",
          "en",
          "license:mit",
          "text-generation-inference",
          "endpoints_compatible",
          "deploy:azure",
          "region:us"
        ],
        "model_id": "microsoft/phi-2",
        "author": "microsoft",
        "last_modified": "2025-12-08T11:35:44.000Z"
      },
      "specifications": {
        "context_length": "2048",
        "architecture": "a Transformer-based model with next-word prediction objective"
      },
      "training": {
        "datasets_description": "data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.",
        "training_tokens": "250T"
      }
    },
    {
      "name": "deepseek-coder-33b-instruct-q4_k_m",
      "description": "DeepSeek Coder 33B Instruct (Quantized)",
      "size": "19 GB",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "TheBloke/deepseek-coder-33B-instruct-GGUF",
      "recommended_for": "code generation",
      "gpu_memory": "22GB",
      "orchestration_categories": [
        "code"
      ],
      "agent_types": [
        "inference"
      ],
      "benchmarks": {},
      "hf_metadata": {
        "downloads": 9536,
        "likes": 192,
        "license": "",
        "languages": [],
        "datasets": [],
        "pipeline_tag": "",
        "tags": [
          "transformers",
          "gguf",
          "deepseek",
          "base_model:deepseek-ai/deepseek-coder-33b-instruct",
          "base_model:quantized:deepseek-ai/deepseek-coder-33b-instruct",
          "license:other",
          "region:us"
        ],
        "model_id": "TheBloke/deepseek-coder-33B-instruct-GGUF",
        "author": "TheBloke",
        "last_modified": "2023-11-05T16:52:39.000Z"
      },
      "specifications": {
        "parameters": "33B",
        "architecture": "deepseek"
      },
      "training": {
        "datasets_description": "Data**: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.",
        "training_tokens": "2T"
      }
    },
    {
      "name": "Llama-3.3-70B-Instruct-Q4_K_M",
      "description": "Meta Llama 3.3 70B Instruct (Quantized)",
      "size": "43 GB",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "meta-llama/Llama-3.3-70B-Instruct-GGUF",
      "recommended_for": "advanced reasoning",
      "gpu_memory": "48GB",
      "notes": "Requires multiple GPUs or CPU offloading on T4",
      "agent_types": [
        "inference"
      ]
    },
    {
      "name": "translategemma-27b-it-GGUF",
      "description": "Google TranslateGemma 27B Instruction-Tuned - Multilingual Translation Model",
      "size": "16.6 GB (Q4_K_M), 22.3 GB (Q6_K), 28.8 GB (Q8_0)",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "mradermacher/translategemma-27b-it-GGUF",
      "original_repo": "google/translategemma-27b-it",
      "recommended_for": "multilingual translation",
      "gpu_memory": "20GB (Q4_K_M), 24GB (Q6_K), 32GB (Q8_0)",
      "variants": [
        "Q2_K",
        "Q3_K_M",
        "Q4_K_M",
        "Q5_K_M",
        "Q6_K",
        "Q8_0"
      ],
      "supported_languages": [
        "ar",
        "bn",
        "cs",
        "da",
        "de",
        "el",
        "en",
        "es",
        "fa",
        "fi",
        "fil",
        "fr",
        "he",
        "hi",
        "hr",
        "hu",
        "id",
        "it",
        "ja",
        "ko",
        "lt",
        "lv",
        "mr",
        "nl",
        "no",
        "pl",
        "pt",
        "ro",
        "ru",
        "sk",
        "sl",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tr",
        "uk",
        "ur",
        "vi",
        "zh"
      ],
      "orchestration_categories": [
        "translation",
        "relational"
      ],
      "agent_types": [
        "inference"
      ],
      "benchmarks": {
        "flores200_avg": "state-of-the-art",
        "languages_supported": 55
      },
      "hf_metadata": {
        "downloads": 2983,
        "likes": 1,
        "license": "",
        "languages": [],
        "datasets": [],
        "pipeline_tag": "",
        "tags": [
          "transformers",
          "gguf",
          "en",
          "base_model:google/translategemma-27b-it",
          "base_model:quantized:google/translategemma-27b-it",
          "license:gemma",
          "endpoints_compatible",
          "region:us",
          "conversational"
        ],
        "model_id": "mradermacher/translategemma-27b-it-GGUF",
        "author": "mradermacher",
        "last_modified": "2026-01-16T10:44:57.000Z"
      },
      "prompt_template": "<source_lang>{source_lang}</source_lang><target_lang>{target_lang}</target_lang>{text}",
      "specifications": {
        "parameters": "27b"
      }
    }
  ],
  "recommended_for_t4": [
    "google-gemma-3-270m-it",
    "LFM2.5-1.2B-Instruct-GGUF",
    "HY-MT1.5-7B",
    "microsoft-phi-2"
  ],
  "recommended_for_a100": [
    "translategemma-27b-it-GGUF",
    "deepseek-coder-33b-instruct-q4_k_m",
    "Llama-3.3-70B-Instruct-Q4_K_M"
  ],
  "translation_models": [
    "translategemma-27b-it-GGUF",
    "HY-MT1.5-7B"
  ],
  "testing_models": [
    "google-gemma-3-270m-it",
    "LFM2.5-1.2B-Instruct-GGUF"
  ]
}