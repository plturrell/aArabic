{
  "models": [
    {
      "name": "google-gemma-3-270m-it",
      "description": "Google Gemma 3 270M Instruction-Tuned (for testing)",
      "size": "540 MB",
      "format": "safetensors",
      "source": "huggingface",
      "hf_repo": "google/gemma-3-270m-it",
      "recommended_for": "testing, development",
      "gpu_memory": "2GB"
    },
    {
      "name": "LFM2.5-1.2B-Instruct-GGUF",
      "description": "Liquid Foundation Model 2.5 1.2B Instruction-Tuned",
      "size": "1.2 GB (Q4), 2.4 GB (F16)",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "LiquidAI/LFM-2.5-1.2B-Instruct-GGUF",
      "recommended_for": "production, inference",
      "gpu_memory": "4GB",
      "variants": ["Q4_0", "Q4_K_M", "F16"]
    },
    {
      "name": "HY-MT1.5-7B",
      "description": "Arabic Machine Translation Model 7B",
      "size": "4.2 GB (Q4), 7.6 GB (Q8)",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "Helsinki-NLP/opus-mt-ar-en",
      "recommended_for": "arabic translation",
      "gpu_memory": "8GB",
      "variants": ["Q4_K_M", "Q6_K", "Q8_0"]
    },
    {
      "name": "microsoft-phi-2",
      "description": "Microsoft Phi-2 2.7B",
      "size": "5.2 GB",
      "format": "safetensors",
      "source": "huggingface",
      "hf_repo": "microsoft/phi-2",
      "recommended_for": "general purpose",
      "gpu_memory": "6GB"
    },
    {
      "name": "deepseek-coder-33b-instruct-q4_k_m",
      "description": "DeepSeek Coder 33B Instruct (Quantized)",
      "size": "19 GB",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "TheBloke/deepseek-coder-33B-instruct-GGUF",
      "recommended_for": "code generation",
      "gpu_memory": "22GB"
    },
    {
      "name": "Llama-3.3-70B-Instruct-Q4_K_M",
      "description": "Meta Llama 3.3 70B Instruct (Quantized)",
      "size": "43 GB",
      "format": "gguf",
      "source": "huggingface",
      "hf_repo": "meta-llama/Llama-3.3-70B-Instruct-GGUF",
      "recommended_for": "advanced reasoning",
      "gpu_memory": "48GB",
      "notes": "Requires multiple GPUs or CPU offloading on T4"
    }
  ],
  "recommended_for_t4": [
    "google-gemma-3-270m-it",
    "LFM2.5-1.2B-Instruct-GGUF",
    "HY-MT1.5-7B",
    "microsoft-phi-2"
  ],
  "testing_models": [
    "google-gemma-3-270m-it",
    "LFM2.5-1.2B-Instruct-GGUF"
  ]
}
