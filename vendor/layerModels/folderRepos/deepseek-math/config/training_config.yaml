# DeepSeek-Math Extended Training Configuration

# Model Configuration
model:
  # Base model from HuggingFace
  base_model: "deepseek-ai/deepseek-math-7b-instruct"
  # Alternative: "deepseek-ai/deepseek-math-7b-base" for pre-training style
  
  # Model loading options
  load_in_8bit: false
  load_in_4bit: true  # QLoRA for memory efficiency
  torch_dtype: "bfloat16"
  device_map: "auto"
  trust_remote_code: true

# LoRA Configuration (for parameter-efficient fine-tuning)
lora:
  enabled: true
  r: 64                    # LoRA rank
  lora_alpha: 128          # LoRA alpha
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Data
data:
  # Training datasets (JSONL format)
  train_files:
    - "data/generated/extended_math.jsonl"
  
  # Validation dataset
  val_file: "data/custom_math_val.jsonl"
  
  # Test datasets for evaluation
  test_files:
    gsm8k: "../../models/deepseek-math/evaluation/datasets/gsm8k/test.jsonl"
    math: "../../models/deepseek-math/evaluation/datasets/math/test.jsonl"
  
  # Data processing
  max_seq_length: 2048
  preprocessing_num_workers: 4

# Training Configuration
training:
  # Output directory
  output_dir: "./outputs/deepseek-math-extended"
  
  # Training hyperparameters
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  
  # Optimizer
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  
  # Precision
  fp16: false
  bf16: true
  
  # Logging
  logging_steps: 10
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  
  # Evaluation
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  
  # Gradient checkpointing for memory efficiency
  gradient_checkpointing: true
  
  # DeepSpeed (optional)
  deepspeed: null  # Path to deepspeed config if using

# RLHF Configuration (for reinforcement learning stage)
rlhf:
  enabled: true
  
  # Reward model
  reward_model: null  # Use rule-based reward for math
  
  # PPO hyperparameters
  ppo_epochs: 4
  mini_batch_size: 4
  kl_penalty: "kl"
  init_kl_coef: 0.2
  target_kl: 6.0
  
  # Generation config for RL
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9

# Mathematical Operations to Extend
math_operations:
  # Enable specific operation categories
  categories:
    - arithmetic
    - algebra
    - calculus
    - linear_algebra
    - probability
    - number_theory
    - geometry
    - custom
  
  # Custom operations to add
  custom_operations:
    - name: "symbolic_integration"
      description: "Advanced symbolic integration techniques"
      examples_file: "data/symbolic_integration.jsonl"
    
    - name: "differential_equations"
      description: "Solving ODEs and PDEs"
      examples_file: "data/diff_equations.jsonl"
    
    - name: "optimization"
      description: "Mathematical optimization problems"
      examples_file: "data/optimization.jsonl"
    
    - name: "graph_theory"
      description: "Graph algorithms and combinatorics"
      examples_file: "data/graph_theory.jsonl"

# Evaluation Configuration
evaluation:
  # Benchmarks to evaluate on
  benchmarks:
    - gsm8k
    - math
    - minerva_math
  
  # Evaluation settings
  num_samples: null  # null for full evaluation
  temperature: 0.0   # Greedy decoding for evaluation
  max_new_tokens: 512
  
  # Chain-of-thought prompting
  use_cot: true
  cot_prompt_en: "Please reason step by step, and put your final answer within \\boxed{}."
  cot_prompt_zh: "请通过逐步推理来解答问题，并把最终答案放置于\\boxed{}中。"

# Logging and Monitoring
logging:
  # Weights & Biases
  wandb_project: "deepseek-math-extended"
  wandb_entity: null
  
  # TensorBoard
  tensorboard_dir: "./logs/tensorboard"
  
  # Report to
  report_to:
    - "tensorboard"
    # - "wandb"  # Uncomment to enable W&B

# Hardware Configuration
hardware:
  # Number of GPUs
  num_gpus: 1
  
  # Mixed precision
  mixed_precision: "bf16"
  
  # Distributed training
  distributed: false
  local_rank: -1
